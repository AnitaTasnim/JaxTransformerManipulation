HookedModule(
  base_module=LlamaForCausalLM(
    model=HookedModule(
      base_module=LlamaModel(
        embed_tokens=HookedModule(
          base_module=LlamaEmbedding(weight=f32[128256,2048]),
          hook_function=<function hooked.<locals>.<lambda>>,
          __activation_wrapper=Mutable()
        ),
        layers=[
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          ),
          HookedModule(
            base_module=LlamaDecoderLayer(
              self_attn=HookedModule(
                base_module=LlamaSdpaAttention(
                  q_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  k_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  v_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[512,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  o_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  rotary_emb=HookedModule(
                    base_module=LlamaRotaryEmbedding(
                      inv_freq=f32[32], max_seq_len_cached=131072
                    ),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  num_heads=32,
                  num_key_value_heads=8,
                  num_key_value_groups=4,
                  head_dim=64,
                  hidden_size=2048,
                  max_position_embeddings=131072,
                  rope_theta=500000.0
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              mlp=HookedModule(
                base_module=LlamaMLP(
                  gate_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  up_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[8192,2048], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  ),
                  down_proj=HookedModule(
                    base_module=LlamaLinear(weight=f32[2048,8192], bias=None),
                    hook_function=<function hooked.<locals>.<lambda>>,
                    __activation_wrapper=Mutable()
                  )
                ),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              input_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              ),
              post_attention_layernorm=HookedModule(
                base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
                hook_function=<function hooked.<locals>.<lambda>>,
                __activation_wrapper=Mutable()
              )
            ),
            hook_function=<function hooked.<locals>.<lambda>>,
            __activation_wrapper=Mutable()
          )
        ],
        norm=HookedModule(
          base_module=LlamaRMSNorm(weight=f32[2048], eps=1e-05),
          hook_function=<function hooked.<locals>.<lambda>>,
          __activation_wrapper=Mutable()
        )
      ),
      hook_function=<function hooked.<locals>.<lambda>>,
      __activation_wrapper=Mutable()
    ),
    lm_head=HookedModule(
      base_module=LlamaLinear(weight=f32[128256,2048], bias=None),
      hook_function=<function hooked.<locals>.<lambda>>,
      __activation_wrapper=Mutable()
    )
  ),
  hook_function=<function hooked.<locals>.<lambda>>,
  __activation_wrapper=Mutable()
)
